{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YeuAheYyhdZw"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader,TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OhLQ2YSvpiGj"
   },
   "source": [
    "### function that returns a dataset with a specified size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Y_tZ1ymVp0Sf"
   },
   "outputs": [],
   "source": [
    "dataFull = np.loadtxt(open(\"mnist_train_small.csv\", \"rb\"), delimiter=\",\")\n",
    "dataFull[:, 1:] = dataFull[:, 1:] / np.max(dataFull)\n",
    "\n",
    "\n",
    "def makeTheDataset(N, doubleTheData=False):\n",
    "\n",
    "    # get n rows of the data \n",
    "    data = dataFull[:N, 1:]\n",
    "    labels = dataFull[:N, 0]\n",
    "\n",
    "    # make a noisy copy of ALL the data\n",
    "    # if doubleTheData:\n",
    "    #     dataN = data + np.random.random_sample(data.shape) / 2\n",
    "    #     data = np.concatenate((data, dataN), axis=0)\n",
    "    #     labels = np.concatenate((labels, labels), axis=0)\n",
    "\n",
    "    # convert to tensor\n",
    "    dataT = torch.tensor(data).float()\n",
    "    labelsT = torch.tensor(labels).long()\n",
    "\n",
    "    # split into training and devset\n",
    "    train_data, devset_data, train_labels, devset_labels = train_test_split(\n",
    "        dataT, labelsT, train_size=0.9\n",
    "    )\n",
    "\n",
    "    # make a noisy copy of the TRAIN data\n",
    "    if doubleTheData:\n",
    "      train_dataN  = train_data + torch.rand_like(train_data)/2\n",
    "      train_data   = torch.cat((train_data,train_dataN),axis=0)\n",
    "      train_labels = torch.cat((train_labels,train_labels),axis=0)\n",
    "\n",
    "    # convert to tensor datasets\n",
    "    train_data = TensorDataset(train_data, train_labels)\n",
    "    devset_data = TensorDataset(devset_data, devset_labels)\n",
    "\n",
    "    # create dataloaders\n",
    "    batchsize = 32\n",
    "    train_loader = DataLoader(train_data, batch_size=batchsize, shuffle=True, drop_last=True)\n",
    "    devset_loader = DataLoader(devset_data, batch_size=devset_data.tensors[0].shape[0])\n",
    "\n",
    "    # Create a test set (don't need a dataloader)\n",
    "    testdata = torch.tensor(dataFull[N:, 1:]).float()\n",
    "    testlabels = torch.tensor(dataFull[N:, 0]).long()\n",
    "\n",
    "    return train_loader, devset_loader, (testdata, testlabels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### visualize the images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rQQAEnLDBeNm"
   },
   "outputs": [],
   "source": [
    "train_loader, devset_loader, testdataset = makeTheDataset(12, True)\n",
    "\n",
    "# pop out the data matrices\n",
    "img = train_loader.dataset.tensors[0].detach()\n",
    "\n",
    "# show the numbers\n",
    "fig, ax = plt.subplots(3, 4, figsize=(12, 8))\n",
    "for i, ax in enumerate(ax.flatten()):\n",
    "    ax.imshow(np.reshape(img[i, :], (28, 28)), cmap=\"gray\")\n",
    "    ax.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OK8Opkhgp0bO"
   },
   "source": [
    "### ANN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class mnistNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.input = nn.Linear(784, 64)\n",
    "        self.fc1 = nn.Linear(64, 32)\n",
    "        self.fc2 = nn.Linear(32, 32)\n",
    "        self.output = nn.Linear(32, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.input(x))\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        return self.output(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JK3OO3tAtZkA"
   },
   "outputs": [],
   "source": [
    "def create_model():\n",
    "    net = mnistNet()\n",
    "    lossfun = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.SGD(net.parameters(), lr=0.01)\n",
    "    return net, lossfun, optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dvfGQIRGp0ht"
   },
   "source": [
    "### function that trains the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IblJo1NCp0kl"
   },
   "outputs": [],
   "source": [
    "def function2trainTheModel():\n",
    "    numepochs = 50\n",
    "    net, lossfun, optimizer = create_model()\n",
    "    losses = torch.zeros(numepochs)\n",
    "    trainAcc = []\n",
    "    devsetAcc = []\n",
    "\n",
    "    for epochi in range(numepochs):\n",
    "        batchAcc = []\n",
    "        batchLoss = []\n",
    "        for X, y in train_loader:\n",
    "\n",
    "            # forward pass and loss\n",
    "            yHat = net(X)\n",
    "            loss = lossfun(yHat, y)\n",
    "\n",
    "            # backprop\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # loss from this batch\n",
    "            batchLoss.append(loss.item())\n",
    "\n",
    "            # compute accuracy\n",
    "            batchAcc.append(100 * torch.mean((torch.argmax(yHat, axis=1) == y).float()))\n",
    "\n",
    "        # average training accuracy\n",
    "        trainAcc.append(np.mean(batchAcc))\n",
    "\n",
    "        # and get average losses across the batches\n",
    "        losses[epochi] = np.mean(batchLoss)\n",
    "\n",
    "        # devset accuracy\n",
    "        X, y = next(iter(devset_loader))\n",
    "        with torch.no_grad():\n",
    "            yHat = net(X)\n",
    "\n",
    "        devsetAcc.append(100 * torch.mean((torch.argmax(yHat, axis=1) == y).float()))\n",
    "\n",
    "    return trainAcc, devsetAcc, losses, net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XpGm9xdQ27Ob"
   },
   "source": [
    "### Run an experiment \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "l9pCC1R2p0nu"
   },
   "outputs": [],
   "source": [
    "samplesizes = np.arange(500, 4001, 500)\n",
    "resultsSingle = np.zeros((len(samplesizes), 3))\n",
    "resultsDouble = np.zeros((len(samplesizes), 3))\n",
    "\n",
    "\n",
    "for idx, ssize in enumerate(samplesizes):\n",
    "\n",
    "    ### without doubling the data!\n",
    "    # generate a dataset and train the model\n",
    "    train_loader, devset_loader, testdataset = makeTheDataset(ssize, False)\n",
    "    trainAcc, devsetAcc, losses, net = function2trainTheModel()\n",
    "\n",
    "    # grab the results\n",
    "    resultsSingle[idx, 0] = np.mean(trainAcc[-5:])\n",
    "    resultsSingle[idx, 1] = np.mean(devsetAcc[-5:])\n",
    "    resultsSingle[idx, 2] = torch.mean(losses[-5:]).item()\n",
    "\n",
    "    ### with doubling the data!\n",
    "    # generate a dataset and train the model\n",
    "    train_loader, devset_loader, testdataset = makeTheDataset(ssize, True)\n",
    "    trainAcc, devsetAcc, losses, net = function2trainTheModel()\n",
    "\n",
    "    # grab the results\n",
    "    resultsDouble[idx, 0] = np.mean(trainAcc[-5:])\n",
    "    resultsDouble[idx, 1] = np.mean(devsetAcc[-5:])\n",
    "    resultsDouble[idx, 2] = torch.mean(losses[-5:]).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9SnUUHPm7xQE"
   },
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(1,3,figsize=(15,5))\n",
    "\n",
    "titles    = ['Train','Devset','Losses']\n",
    "yaxlabels = ['Accuracy','Accuracy','Losses']\n",
    "\n",
    "for i in range(3):\n",
    "\n",
    "  # plot the lines\n",
    "  ax[i].plot(samplesizes,resultsSingle[:,i],'s-',label='Original')\n",
    "  ax[i].plot(samplesizes,resultsDouble[:,i],'s-',label='Augmented')\n",
    "\n",
    "  # make it look nicer\n",
    "  ax[i].set_ylabel(yaxlabels[i])\n",
    "  ax[i].set_title(titles[i])\n",
    "  ax[i].legend()\n",
    "  ax[i].set_xlabel('Unique sample size')\n",
    "  ax[i].grid('on')\n",
    "\n",
    "  if i<2:\n",
    "    ax[i].set_ylim([20,102])\n",
    "\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2QVdrgkBHPVS"
   },
   "source": [
    "### test the model on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1E6m2P4I_MRP"
   },
   "outputs": [],
   "source": [
    "# we need to re-run the models for N=500\n",
    "samplesize = 500\n",
    "\n",
    "train_loader, devset_loader, testdataset = makeTheDataset(samplesize, False)\n",
    "trainAccO, devsetAccO, lossesO, netO = function2trainTheModel()  # O = original\n",
    "\n",
    "train_loader, devset_loader, testdataset = makeTheDataset(samplesize, True)\n",
    "trainAccA, devsetAccA, lossesA, netA = function2trainTheModel()  # A = augmented"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "p_ezuI0NLP_U"
   },
   "outputs": [],
   "source": [
    "# extract the test data\n",
    "X,y = testdataset\n",
    "\n",
    "# run the original model\n",
    "yHat = netO(X)\n",
    "testO = 100*torch.mean((torch.argmax(yHat,axis=1)==y).float())\n",
    "\n",
    "# And the augmented model\n",
    "yHat = netA(X)\n",
    "testA = 100*torch.mean((torch.argmax(yHat,axis=1)==y).float())\n",
    "\n",
    "# print the results!\n",
    "print( f'ORIGINAL MODEL (N={samplesize}):\\n  Train: {trainAccO[-1]:.2f}%, devset: {devsetAccO[-1]:.2f}%, test: {testO:.2f}%\\n\\n')\n",
    "print(f'AUGMENTED MODEL (N={samplesize}):\\n  Train: {trainAccA[-1]:.2f}%, devset: {devsetAccA[-1]:.2f}%, test: {testA:.2f}%')\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [
    {
     "file_id": "1FcEBC0NAESIlHQkv6_85R-XDUKGE8XbM",
     "timestamp": 1618343530003
    },
    {
     "file_id": "1qKgZ8kVcqNgwtBzHbWq5yJH_HqI6DxWW",
     "timestamp": 1617803880910
    },
    {
     "file_id": "15cpyHkJ435B4MqbyGjAH1poN4nCy_DE4",
     "timestamp": 1617737766196
    },
    {
     "file_id": "1OLuWuaFu0hcFgkQ2hh5BqbRuqUZD7XcQ",
     "timestamp": 1617734878578
    },
    {
     "file_id": "1XvzVGJPTJifVh8OpZVB7ykLxyUqYwQ1j",
     "timestamp": 1617196833019
    },
    {
     "file_id": "1bv1_y32e3KEExFKKlPfC3rpw1JxmBr8H",
     "timestamp": 1617124341706
    },
    {
     "file_id": "1GMq8u7KyHB2AE7Teyls9gK1T01OduQSn",
     "timestamp": 1616697516760
    },
    {
     "file_id": "1Ui3kyHim-e0XLgDs2mkBxVlYg7TKYtcg",
     "timestamp": 1616615469755
    },
    {
     "file_id": "1YpHocGI4rApOxIBb1ZghCU5L-hFnv4CK",
     "timestamp": 1616608248670
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
