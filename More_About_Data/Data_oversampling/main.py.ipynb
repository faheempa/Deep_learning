{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bhWV8oes-wKR"
   },
   "source": [
    "### imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YeuAheYyhdZw"
   },
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OhLQ2YSvpiGj"
   },
   "source": [
    "### A function that returns a dataset with a specified size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Y_tZ1ymVp0Sf"
   },
   "outputs": [],
   "source": [
    "dataFull = np.loadtxt(open(\"mnist_train_small.csv\", \"rb\"), delimiter=\",\")\n",
    "\n",
    "\n",
    "# now for the function\n",
    "def makeTheDataset(N, doubleTheData=False):\n",
    "\n",
    "    # extract labels (number IDs) and remove from data\n",
    "    labels = dataFull[:N, 0]\n",
    "    data = dataFull[:N, 1:]\n",
    "\n",
    "    # normalize the data to a range of [0 1]\n",
    "    dataNorm = data / np.max(data)\n",
    "\n",
    "    # make an exact copy of ALL the data\n",
    "    # if doubleTheData:\n",
    "    #   dataNorm = np.concatenate((dataNorm,dataNorm),axis=0)\n",
    "    #   labels   = np.concatenate((labels,labels),axis=0)\n",
    "\n",
    "    # convert to tensor\n",
    "    dataT = torch.tensor(dataNorm).float()\n",
    "    labelsT = torch.tensor(labels).long()\n",
    "\n",
    "    # use scikitlearn to split the data\n",
    "    train_data, test_data, train_labels, test_labels = train_test_split(\n",
    "        dataT, labelsT, train_size=0.9\n",
    "    )\n",
    "\n",
    "    # # make an exact copy of the TRAIN data\n",
    "    if doubleTheData:\n",
    "        train_data = torch.cat((train_data, train_data), axis=0)\n",
    "        train_labels = torch.cat((train_labels, train_labels), axis=0)\n",
    "\n",
    "    # convert into PyTorch Datasets\n",
    "    train_data = TensorDataset(train_data, train_labels)\n",
    "    test_data = TensorDataset(test_data, test_labels)\n",
    "\n",
    "    # translate into dataloader objects\n",
    "    batchsize = 20\n",
    "    train_loader = DataLoader(\n",
    "        train_data, batch_size=batchsize, shuffle=True, drop_last=True\n",
    "    )\n",
    "    test_loader = DataLoader(test_data, batch_size=test_data.tensors[0].shape[0])\n",
    "\n",
    "    return train_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rQQAEnLDBeNm"
   },
   "outputs": [],
   "source": [
    "# Check the sizes\n",
    "r, t = makeTheDataset(200, False)\n",
    "print(r.dataset.tensors[0].shape)\n",
    "print(t.dataset.tensors[0].shape)\n",
    "\n",
    "r, t = makeTheDataset(200, True)\n",
    "print(r.dataset.tensors[0].shape)\n",
    "print(t.dataset.tensors[0].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OK8Opkhgp0bO"
   },
   "source": [
    "### ANN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JK3OO3tAtZkA"
   },
   "outputs": [],
   "source": [
    "class mnistNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        ### input layer\n",
    "        self.input = nn.Linear(784, 64)\n",
    "\n",
    "        ### hidden layer\n",
    "        self.fc1 = nn.Linear(64, 32)\n",
    "        self.fc2 = nn.Linear(32, 32)\n",
    "\n",
    "        ### output layer\n",
    "        self.output = nn.Linear(32, 10)\n",
    "\n",
    "    # forward pass\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.input(x))\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        return self.output(x)\n",
    "\n",
    "\n",
    "def create_model():\n",
    "    net = mnistNet()\n",
    "    lossfun = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.SGD(net.parameters(), lr=0.01)\n",
    "    return net, lossfun, optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dvfGQIRGp0ht"
   },
   "source": [
    "### Create a function that trains the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IblJo1NCp0kl"
   },
   "outputs": [],
   "source": [
    "def function2trainTheModel(train_loader, test_loader):\n",
    "\n",
    "    numepochs = 50\n",
    "    net, lossfun, optimizer = create_model()\n",
    "    losses = torch.zeros(numepochs)\n",
    "    trainAcc = []\n",
    "    testAcc = []\n",
    "\n",
    "    for epochi in range(numepochs):\n",
    "        batchAcc = []\n",
    "        batchLoss = []\n",
    "        for X, y in train_loader:\n",
    "\n",
    "            # forward pass and loss\n",
    "            yHat = net(X)\n",
    "            loss = lossfun(yHat, y)\n",
    "\n",
    "            # backprop\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # loss from this batch\n",
    "            batchLoss.append(loss.item())\n",
    "\n",
    "            # compute accuracy\n",
    "            batchAcc.append(100 * torch.mean((torch.argmax(yHat, axis=1) == y).float()))\n",
    "\n",
    "        # average training accuracy\n",
    "        trainAcc.append(np.mean(batchAcc))\n",
    "\n",
    "        # and get average losses across the batches\n",
    "        losses[epochi] = np.mean(batchLoss)\n",
    "\n",
    "        # test accuracy\n",
    "        X, y = next(iter(test_loader))\n",
    "        with torch.no_grad():\n",
    "            yHat = net(X)\n",
    "\n",
    "        testAcc.append(100 * torch.mean((torch.argmax(yHat, axis=1) == y).float()))\n",
    "\n",
    "    return trainAcc, testAcc, losses, net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rkI7gcZH26m2"
   },
   "source": [
    "### Run the model once to confirm that it works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gHzKOZjnp0qn"
   },
   "outputs": [],
   "source": [
    "train_loader,test_loader = makeTheDataset(5000)\n",
    "trainAcc,testAcc,losses,net = function2trainTheModel()\n",
    "\n",
    "# plot the results\n",
    "fig,ax = plt.subplots(1,2,figsize=(16,5))\n",
    "\n",
    "ax[0].plot(losses)\n",
    "ax[0].set_xlabel('Epochs')\n",
    "ax[0].set_ylabel('Loss')\n",
    "ax[0].set_ylim([0,3])\n",
    "ax[0].set_title('Model loss')\n",
    "\n",
    "ax[1].plot(trainAcc,label='Train')\n",
    "ax[1].plot(testAcc,label='Test')\n",
    "ax[1].set_xlabel('Epochs')\n",
    "ax[1].set_ylabel('Accuracy (%)')\n",
    "ax[1].set_ylim([10,100])\n",
    "ax[1].set_title(f'Final model test accuracy: {testAcc[-1]:.2f}%')\n",
    "ax[1].legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XpGm9xdQ27Ob"
   },
   "source": [
    "### Run an experiment \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "l9pCC1R2p0nu"
   },
   "outputs": [],
   "source": [
    "# list of data sample sizes\n",
    "samplesizes = np.arange(500, 4001, 500)\n",
    "\n",
    "# initialize results matrix\n",
    "resultsSingle = np.zeros((len(samplesizes), 3))\n",
    "resultsDouble = np.zeros((len(samplesizes), 3))\n",
    "\n",
    "\n",
    "for idx, ssize in enumerate(samplesizes):\n",
    "\n",
    "    ### without doubling the data!\n",
    "    train_loader, test_loader = makeTheDataset(ssize, False)\n",
    "    trainAcc, testAcc, losses, net = function2trainTheModel()\n",
    "\n",
    "    # grab the results\n",
    "    resultsSingle[idx, 0] = np.mean(trainAcc[-5:])\n",
    "    resultsSingle[idx, 1] = np.mean(testAcc[-5:])\n",
    "    resultsSingle[idx, 2] = torch.mean(losses[-5:]).item()\n",
    "\n",
    "    ### with doubling the data!\n",
    "    train_loader, test_loader = makeTheDataset(ssize, True)\n",
    "    trainAcc, testAcc, losses, net = function2trainTheModel()\n",
    "\n",
    "    # grab the results\n",
    "    resultsDouble[idx, 0] = np.mean(trainAcc[-5:])\n",
    "    resultsDouble[idx, 1] = np.mean(testAcc[-5:])\n",
    "    resultsDouble[idx, 2] = torch.mean(losses[-5:]).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9SnUUHPm7xQE"
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "# axis and title labels\n",
    "titles = [\"Train\", \"Devset\", \"Losses\"]\n",
    "yaxlabels = [\"Accuracy\", \"Accuracy\", \"Losses\"]\n",
    "\n",
    "for i in range(3):\n",
    "\n",
    "    # plot the lines\n",
    "    ax[i].plot(samplesizes, resultsSingle[:, i], \"s-\", label=\"Original\")\n",
    "    ax[i].plot(samplesizes, resultsDouble[:, i], \"s-\", label=\"Doubled\")\n",
    "\n",
    "    # make it look nicer\n",
    "    ax[i].set_ylabel(yaxlabels[i])\n",
    "    ax[i].set_title(titles[i])\n",
    "    ax[i].legend()\n",
    "    ax[i].set_xlabel(\"Unique sample size\")\n",
    "    ax[i].grid(\"on\")\n",
    "\n",
    "    if i < 2:\n",
    "        ax[i].set_ylim([20, 102])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMfIbRLNAjprsUyyiYQOHY5",
   "collapsed_sections": [],
   "name": "DUDL_data_oversampling.ipynb",
   "provenance": [
    {
     "file_id": "1FcEBC0NAESIlHQkv6_85R-XDUKGE8XbM",
     "timestamp": 1618343530003
    },
    {
     "file_id": "1qKgZ8kVcqNgwtBzHbWq5yJH_HqI6DxWW",
     "timestamp": 1617803880910
    },
    {
     "file_id": "15cpyHkJ435B4MqbyGjAH1poN4nCy_DE4",
     "timestamp": 1617737766196
    },
    {
     "file_id": "1OLuWuaFu0hcFgkQ2hh5BqbRuqUZD7XcQ",
     "timestamp": 1617734878578
    },
    {
     "file_id": "1XvzVGJPTJifVh8OpZVB7ykLxyUqYwQ1j",
     "timestamp": 1617196833019
    },
    {
     "file_id": "1bv1_y32e3KEExFKKlPfC3rpw1JxmBr8H",
     "timestamp": 1617124341706
    },
    {
     "file_id": "1GMq8u7KyHB2AE7Teyls9gK1T01OduQSn",
     "timestamp": 1616697516760
    },
    {
     "file_id": "1Ui3kyHim-e0XLgDs2mkBxVlYg7TKYtcg",
     "timestamp": 1616615469755
    },
    {
     "file_id": "1YpHocGI4rApOxIBb1ZghCU5L-hFnv4CK",
     "timestamp": 1616608248670
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
